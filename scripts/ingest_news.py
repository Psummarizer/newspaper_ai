"""
Hourly Process Pipeline - OPTIMIZED
====================================
Mejoras implementadas:
1. Paralelismo por Topics (asyncio.Semaphore)
2. Cache compartido entre Topics (misma categor√≠a)
3. Deduplicaci√≥n sem√°ntica (evitar redactar la misma noticia dos veces)
"""

import asyncio
import sys
import os
import logging
import json
import re
from datetime import datetime, timedelta
from dotenv import load_dotenv

load_dotenv()

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import aiohttp
import feedparser
from openai import AsyncOpenAI
from src.services.gcs_service import GCSService
from src.services.firebase_service import FirebaseService
from src.utils.html_builder import CATEGORY_IMAGES

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def _extract_json(text: str) -> dict:
    """Extract JSON from LLM response that may contain markdown code blocks."""
    text = re.sub(r'^```json\s*', '', text.strip())
    text = re.sub(r'^```\s*', '', text)
    text = re.sub(r'\s*```$', '', text)
    text = text.strip()
    return json.loads(text)

# Lista oficial de categor√≠as
VALID_CATEGORIES = list(CATEGORY_IMAGES.keys())

# Configuraci√≥n de paralelismo
MAX_CONCURRENT_TOPICS = 5
MAX_CONCURRENT_REDACTIONS = 3


class HourlyProcessor:
    def __init__(self):
        self.client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.model = "gpt-5-nano"
        self.gcs = GCSService()
        self.fb = FirebaseService()
        
        # Caches compartidos (thread-safe via asyncio)
        self.redacted_cache = {}  # {"url": redacted_news_dict}
        self.category_news_cache = {}  # {"Deporte": [redacted_news_list]}
        self.existing_news = {}  # {normalized_title: {"news": news_dict, "topic_id": str}}
        self.existing_urls = set() # {url} para de-duplicaci√≥n estricta
        
    async def run(self):
        logger.info("üöÄ Inicio Pipeline Horario (OPTIMIZADO)")
        
        # 0. Load State for Dynamic Window
        self.last_run_time = None
        state = self.gcs.get_json_file("ingest_state.json")
        if state and state.get("last_run_finished"):
            try:
                self.last_run_time = datetime.fromisoformat(state.get("last_run_finished"))
                ago = datetime.now() - self.last_run_time
                logger.info(f"üïí √öltima ejecuci√≥n: {self.last_run_time} (hace {ago})")
            except Exception as e:
                logger.warning(f"Error parseando last_run_time: {e}")
        
        # 0. INGESTA RSS
        await self._ingest_all_rss()
        
        # 0.1 LIMPIEZA DE DATOS ANTIGUOS
        removed_articles = self.gcs.cleanup_old_articles(hours=168)  # 7 d√≠as
        if removed_articles > 0:
            logger.info(f"üßπ Eliminados {removed_articles} art√≠culos antiguos (>7 d√≠as)")
        
        # 1. Obtener todos los aliases √∫nicos de Firebase
        all_aliases_tuples = self._get_all_topics_from_firebase()
        all_aliases = [t[0] for t in all_aliases_tuples]
        logger.info(f"üìã Aliases de usuarios encontrados: {len(all_aliases)}")
        
        # 2. Cargar topics.json actual
        topics_data = self._load_topics_json()
        logger.info(f"üì¶ Topics existentes: {len(topics_data)}")
        
        # 2.1 Limpiar noticias antiguas de topics (>7 d√≠as)
        removed_news = self.gcs.cleanup_old_topic_news(topics_data, days=7)
        if removed_news > 0:
            logger.info(f"üßπ Eliminadas {removed_news} noticias antiguas (>7 d√≠as) de topics")
        
        # 3. Sincronizar aliases con topics (LLM matching sem√°ntico)
        topics_data = await self._sync_aliases_with_topics(all_aliases_tuples, topics_data)
        logger.info(f"üîÑ Topics despu√©s de sincronizaci√≥n: {len(topics_data)}")
        
        # Cargar noticias existentes para deduplicaci√≥n sem√°ntica
        self._load_existing_news(topics_data)
        
        # 4. Procesar Topics EN PARALELO (con l√≠mite)
        semaphore = asyncio.Semaphore(MAX_CONCURRENT_TOPICS)
        
        # FILTRO DE ACTIVIDAD: Solo procesar topics que tengan al menos un alias ACTIVO
        # (Esto evita procesar topics de usuarios que han cancelado)
        active_aliases_set = set(all_aliases) # all_aliases now only contains active users' topics
        topic_names = []
        
        for tid, info in topics_data.items():
            t_candidates = info.get("aliases", []) + [info.get("name", "")]
            # Normalizar para comparaci√≥n
            is_active = False
            for cand in t_candidates:
                if cand in active_aliases_set:
                    is_active = True
                    break
            
            if is_active:
                topic_names.append(info.get("name", tid))
            else:
                logger.info(f"üí§ Topic '{info.get('name')}' SALTADO (Inactivo/Sin usuarios)")
        logger.info(f"üì∞ Procesando {len(topic_names)} topics...")
        
        async def process_topic_wrapper(topic_name):
            async with semaphore:
                return await self._process_single_topic(topic_name, topics_data)
        
        tasks = [process_topic_wrapper(topic) for topic in topic_names]
        
        # PROCESAMIENTO INCREMENTAL Y GUARDADO
        # Usamos as_completed para ir guardando conforme terminan los topics
        # Esto previene que un timeout total nos deje sin NADA de datos.
        msg_counter = 0
        for future in asyncio.as_completed(tasks):
            try:
                result = await future
                if isinstance(result, dict):
                    topic_id = result.get("topic_id")
                    if topic_id:
                        topics_data[topic_id] = result.get("data")
                        msg_counter += 1
                        
                        # Guardar cada 1 topic (o ajustar si es mucho I/O, pero GCS aguanta)
                        # Para m√°xima seguridad: Guardar SIEMPRE
                        logger.info(f"üíæ Guardado incremental ({msg_counter}/{len(topic_names)}): {topic_id}")
                        self._save_topics_json(topics_data)
            except Exception as e:
                logger.error(f"‚ùå Error en topic task: {e}")
        
        # 4. Guardar topics.json
        self._save_topics_json(topics_data)
        logger.info("üíæ topics.json actualizado")
        
        # Stats
        total_redacted = sum(1 for v in self.redacted_cache.values() if v)
        logger.info(f"üìä Stats: {total_redacted} noticias redactadas, {len(self.existing_news)} en cache de dedup")
        
        # 5. Guardar estado de finalizaci√≥n
        self.gcs.save_json_file("ingest_state.json", {
            "last_run_finished": datetime.now().isoformat()
        })
        logger.info("üíæ Estado guardado (ingest_state.json)")
        
    def _load_existing_news(self, topics_data: dict):
        """Carga noticias existentes para deduplicaci√≥n sem√°ntica (con referencia completa)"""
        for topic_id, topic_info in topics_data.items():
            for idx, news in enumerate(topic_info.get("noticias", [])):
                
                # Check sources URLs
                for src in news.get("fuentes", []):
                    if src: self.existing_urls.add(src)
                
                title = news.get("titulo", "")
                if title:
                    normalized = self._normalize_title(title)
                    self.existing_news[normalized] = {
                        "news": news,
                        "topic_id": topic_id,
                        "index": idx
                    }
        logger.info(f"üìö Cargadas {len(self.existing_news)} noticias para deduplicaci√≥n (URLs conocidas: {len(self.existing_urls)})")
                    
    def _normalize_title(self, title: str) -> str:
        """Normaliza t√≠tulo para comparaci√≥n (quita emojis, espacios, etc.)"""
        # Quitar emojis comunes
        import re
        title = re.sub(r'[^\w\s]', '', title.lower())
        title = re.sub(r'\s+', ' ', title).strip()
        return title[:80]  # Primeros 80 chars
        
    async def _process_single_topic(self, topic_name: str, topics_data: dict) -> dict:
        """Procesa un topic individual"""
        topic_id = self._normalize_id(topic_name)
        
        # Inicializar si no existe
        if topic_id not in topics_data:
            topics_data[topic_id] = {
                "name": topic_name,
                "aliases": [topic_name],
                "categories": [],
                "noticias": []
            }
        
        topic_info = topics_data[topic_id]
        
        # Asignar categor√≠as si no tiene
        if not topic_info.get("categories"):
            categories = await self._assign_categories(topic_name)
            topic_info["categories"] = categories
            logger.info(f"üìÇ {topic_name} ‚Üí {categories}")
        
        categories = topic_info["categories"]
        
        # MEJORA B: Buscar noticias ya redactadas de la misma categor√≠a
        cached_news = self._get_cached_news_for_categories(categories)
        if cached_news:
            logger.info(f"‚ö° {topic_name}: {len(cached_news)} noticias desde cache de categor√≠a")
        
        # Buscar art√≠culos nuevos
        candidates = self._get_articles_for_categories(categories)
        
        if not candidates and not cached_news:
            logger.info(f"‚è≠Ô∏è {topic_name}: Sin art√≠culos nuevos")
            return {"topic_id": topic_id, "data": topic_info}
        
        logger.info(f"üì• {topic_name}: {len(candidates)} candidatos nuevos")
        
        # Filtrar relevantes
        user_contexts = topic_info.get("user_contexts", [])
        relevant = await self._filter_relevant(topic_name, candidates, user_contexts) if candidates else []
        logger.info(f"‚úÖ {topic_name}: {len(relevant)} relevantes")
        
        # Redactar noticias nuevas (con deduplicaci√≥n)
        redaction_semaphore = asyncio.Semaphore(MAX_CONCURRENT_REDACTIONS)
        
        async def redact_with_dedup(art):
            async with redaction_semaphore:
                return await self._redact_with_deduplication(art, topic_name, categories, topics_data)
        
        # Solo redactar los que no est√°n en cache
        new_to_redact = []
        for art in relevant:
            url = art.get("url", "")
            if url not in self.redacted_cache:
                new_to_redact.append(art)
            else:
                # Ya est√° en cache, reutilizar
                cached = self.redacted_cache[url]
                if cached and cached not in topic_info.get("noticias", []):
                    topic_info["noticias"].append(cached)
        
        if new_to_redact:
            redaction_tasks = [redact_with_dedup(art) for art in new_to_redact]
            redacted_results = await asyncio.gather(*redaction_tasks, return_exceptions=True)
            
            for result in redacted_results:
                if isinstance(result, dict) and result.get("titulo"):
                    topic_info["noticias"].append(result)
                    logger.info(f"‚úçÔ∏è {topic_name}: {result['titulo'][:40]}...")
        
        # A√±adir noticias de cache de categor√≠a si son relevantes
        for cached_news_item in cached_news:
            if cached_news_item not in topic_info.get("noticias", []):
                # Verificar relevancia r√°pida
                if await self._is_relevant_for_topic(cached_news_item, topic_name):
                    topic_info["noticias"].append(cached_news_item)
                    logger.info(f"‚ôªÔ∏è {topic_name}: Reutilizada noticia de cache")
        
        # POST-MERGE: Fusionar noticias similares (procesadas en paralelo)
        topic_info["noticias"] = await self._merge_similar_news(topic_info.get("noticias", []))
        
        return {"topic_id": topic_id, "data": topic_info}
    
    async def _merge_similar_news(self, news_list: list) -> list:
        """
        Fusiona noticias similares que fueron procesadas en paralelo.
        Compara t√≠tulos normalizados y combina fuentes.
        """
        if len(news_list) <= 1:
            return news_list
        
        merged = []
        used_indices = set()
        
        for i, news_a in enumerate(news_list):
            if i in used_indices:
                continue
            
            title_a = self._normalize_title(news_a.get("titulo", ""))
            sources_a = list(news_a.get("fuentes", []))
            
            # Buscar noticias similares
            for j, news_b in enumerate(news_list):
                if j <= i or j in used_indices:
                    continue
                
                title_b = self._normalize_title(news_b.get("titulo", ""))
                
                # Comparar similitud (si > 60% de palabras coinciden, son similares)
                words_a = set(title_a.split())
                words_b = set(title_b.split())
                if not words_a or not words_b:
                    continue
                
                common = len(words_a & words_b)
                similarity = common / max(len(words_a), len(words_b))
                
                # LOWER THRESHOLD from 0.6 to 0.35 to catch more sources for same event
                # e.g. "Madrid gana Supercopa" vs "Real Madrid campe√≥n de Supercopa"
                if similarity > 0.35:
                    # Fusionar fuentes
                    for src in news_b.get("fuentes", []):
                        if src and src not in sources_a:
                            sources_a.append(src)
                    used_indices.add(j)
                    logger.info(f"üîó Fusionadas fuentes (sim={similarity:.2f}): {news_a.get('titulo', '')[:40]}... ({len(sources_a)} fuentes)")
            
            # Actualizar fuentes y a√±adir
            news_a["fuentes"] = sources_a[:5]  # Max 5 fuentes
            merged.append(news_a)
            used_indices.add(i)
        
        return merged
    
    def _get_cached_news_for_categories(self, categories: list) -> list:
        """Busca noticias ya redactadas para las categor√≠as dadas"""
        result = []
        for cat in categories:
            if cat in self.category_news_cache:
                result.extend(self.category_news_cache[cat])
        return result
    
    async def _redact_with_deduplication(self, article: dict, topic: str, categories: list, topics_data: dict = None) -> dict:
        """Redacta con verificaci√≥n de duplicados sem√°nticos y detecci√≥n de actualizaciones"""
        url = article.get("url", "")
        title = article.get("title", "")
        content = article.get("content", "")
        
        # Check 1: URL ya procesada (Cache ef√≠mera O Hist√≥rica)
        if url in self.redacted_cache:
            return self.redacted_cache[url]
        
        if url in self.existing_urls:
            # Ya existe en el hist√≥rico -> Es un DUPLICADO exacto
            # Retornamos None para que no se vuelva a a√±adir (ni siquiera gastamos LLM en dedup check)
            logger.info(f"‚è≠Ô∏è URL ya existente (Hist√≥rico): {url[:40]}...")
            self.redacted_cache[url] = None 
            return None
        
        # Check 2: Deduplicaci√≥n sem√°ntica con detecci√≥n de actualizaciones
        dedup_result = await self._check_duplicate_or_update(title, content)
        status = dedup_result.get("status", "different")
        matched_key = dedup_result.get("matched_key")
        
        if status == "duplicate" and matched_key:
            # Es DUPLICADO - a√±adir esta URL a las fuentes de la noticia existente
            logger.info(f"üîÑ Duplicado detectado: a√±adiendo fuente a '{matched_key[:40]}...'")
            existing_info = self.existing_news.get(matched_key)
            if existing_info and existing_info.get("news"):
                existing_news = existing_info["news"]
                existing_sources = existing_news.get("fuentes", [])
                if url and url not in existing_sources:
                    existing_sources.append(url)
                    existing_news["fuentes"] = existing_sources[:5]  # Max 5 fuentes
                    logger.info(f"   ‚úÖ Fuentes ahora: {len(existing_news['fuentes'])}")
            self.redacted_cache[url] = None
            return None
        
        elif status == "update" and matched_key and topics_data:
            # Es una ACTUALIZACI√ìN - redactar y reemplazar la vieja
            logger.info(f"üìù ACTUALIZACI√ìN detectada: {title[:50]}...")
            redacted = await self._redact_article(article, topic)
            
            if redacted:
                # Reemplazar la noticia vieja
                old_info = self.existing_news.get(matched_key)
                if old_info:
                    old_topic_id = old_info["topic_id"]
                    old_index = old_info["index"]
                    if old_topic_id in topics_data:
                        noticias = topics_data[old_topic_id].get("noticias", [])
                        if old_index < len(noticias):
                            noticias[old_index] = redacted
                            logger.info(f"‚ôªÔ∏è Reemplazada noticia antigua en {old_topic_id}")
                
                # Actualizar caches
                self.redacted_cache[url] = redacted
                self.existing_news[self._normalize_title(redacted.get("titulo", ""))] = {
                    "news": redacted,
                    "topic_id": self._normalize_id(topic),
                    "index": -1  # Se actualizar√° al guardar
                }
            return redacted
        
        # status == "different" - Noticia nueva, redactar normalmente
        redacted = await self._redact_article(article, topic)
        
        if redacted:
            self.redacted_cache[url] = redacted
            
            for cat in categories:
                if cat not in self.category_news_cache:
                    self.category_news_cache[cat] = []
                self.category_news_cache[cat].append(redacted)
            
            self.existing_news[self._normalize_title(redacted.get("titulo", ""))] = {
                "news": redacted,
                "topic_id": self._normalize_id(topic),
                "index": -1
            }
        
        return redacted
    
    async def _check_duplicate_or_update(self, new_title: str, new_content: str) -> dict:
        """
        Detecta si la noticia es:
        - 'duplicate': Misma noticia, sin info nueva -> SKIP
        - 'update': Misma noticia pero con M√ÅS informaci√≥n (Resultados, Confirmaciones) -> REEMPLAZAR
        - 'different': Noticia diferente -> REDACTAR
        
        Returns: {"status": str, "matched_key": str or None}
        """
        if not self.existing_news:
            return {"status": "different", "matched_key": None}
        
        # Check r√°pido: t√≠tulo normalizado exacto
        normalized_new = self._normalize_title(new_title)
        if normalized_new in self.existing_news:
            return {"status": "duplicate", "matched_key": normalized_new}
        
        # Check con LLM solo si hay suficientes noticias existentes
        if len(self.existing_news) < 3:
            return {"status": "different", "matched_key": None}
        
        # Tomar muestra de t√≠tulos existentes CON RESUMEN (Contexto enriquecido)
        sample_keys = list(self.existing_news.keys())
        # Limitar a las √∫ltimas 15 para no saturar el prompt, priorizando las m√°s recientes si hay logica temporal
        sample_keys = sample_keys[:15]
        
        titles_text = ""
        for i, k in enumerate(sample_keys):
            news_item = self.existing_news[k].get("news", {})
            existing_title = news_item.get("titulo", k)
            existing_summary = news_item.get("resumen", "")[:300].replace("\n", " ")
            titles_text += f"ID_{i}:\nT√çTULO: {existing_title}\nRESUMEN: {existing_summary}\n---\n"
        
        prompt = f"""
        Act√∫a como un Editor Jefe Inteligente.
        Tu tarea es detectar si una NOTICIA NUEVA se solapa con alguna YA EXISTENTE.

        NOTICIA NUEVA:
        Titulo: {new_title}
        Contenido: {new_content[:500]}
        
        NOTICIAS YA PROCESADAS (Contexto):
        {titles_text}
        
        INSTRUCCIONES DE L√ìGICA (STRICT):
        Debes comparar la NOTICIA NUEVA con cada ID existente y decidir:

        1. DUPLICATE (Duplicado):
           - Se refiere al MISMO evento, anuncio, declaraci√≥n o hecho principal.
           - Aunque cambie la fuente, el enfoque o algunas palabras, la informaci√≥n central es la misma.
           - Ejemplo abstracto: "X anuncia Y" vs "Y es anunciado por X". -> DUPLICATE.

        2. UPDATE (Actualizaci√≥n / Obsolescencia):
           - La NOTICIA NUEVA contiene informaci√≥n POSTERIOR que hace obsoleta a la anterior.
           - Casos t√≠picos:
             * PREVIA vs RESULTADO (El partido se jug√≥ y hay marcador).
             * RUMOR vs CONFIRMACI√ìN (Oficializaci√≥n de un fichaje o medida).
             * INICIO vs DESENLACE (Una reuni√≥n empez√≥ vs Termin√≥ con acuerdo).
           - Si la nueva noticia aporta el RESULTADO FINAL o un estado M√ÅS AVANZADO -> UPDATE.

        3. DIFFERENT (Diferente):
           - Hechos distintos, personas distintas, o eventos sin relaci√≥n directa.
           - Si son dos partidos diferentes de la misma jornada -> DIFFERENT.
        
        SALIDA ESPERADA (JSON):
        {{"status": "duplicate" | "update" | "different", "matched_id": <numero_id_existente> | null}}
        """
        
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}]
            )
            result = _extract_json(response.choices[0].message.content)
            status = result.get("status", "different")
            matched_id = result.get("matched_id")
            
            matched_key = None
            if matched_id is not None and isinstance(matched_id, int) and 0 <= matched_id < len(sample_keys):
                matched_key = sample_keys[matched_id]
            
            return {"status": status, "matched_key": matched_key}
        except Exception as e:
            logger.warning(f"Error en dedup check: {e}")
            return {"status": "different", "matched_key": None}
    
    async def _is_relevant_for_topic(self, news_item: dict, topic: str) -> bool:
        """Verifica si una noticia cacheada es relevante para el topic"""
        title = news_item.get("titulo", "")
        resumen = news_item.get("resumen", "")
        
        prompt = f"""
        ¬øEsta noticia es relevante para el topic "{topic}"?
        
        T√≠tulo: {title}
        Resumen: {resumen}
        
        Responde JSON: {{"is_relevant": true/false}}
        """
        
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}]
            )
            result = _extract_json(response.choices[0].message.content)
            return result.get("is_relevant", False)
        except:
            return False
    
    # =========================================================================
    # M√âTODOS EXISTENTES (sin cambios significativos)
    # =========================================================================
    
    def _get_all_topics_from_firebase(self) -> list:
        """Lee todos los aliases √∫nicos de usuarios ACTIVOS en AINewspaper. Retorna [(alias, description), ...]"""
        aliases_tuples = set()
        docs = self.fb.db.collection("AINewspaper").stream()
        for doc in docs:
            data = doc.to_dict()
            
            # FILTRO: Solo usuarios activos
            if data.get("is_active") is False:
                continue
            
            # NEW SCHEMA: 'topic' is a Map<Alias, Description>
            # Check 'topic' OR 'topics' in case the user named it either way (Map preference)
            topic_map = data.get("topic") or data.get("topics")
            if isinstance(topic_map, dict):
                for alias, desc in topic_map.items():
                    if alias and alias.strip():
                        aliases_tuples.add((alias.strip(), str(desc or "").strip()))
                continue

            # Fallback: legacy fields (list/string)
            user_topics = data.get("Topics") or data.get("topics", [])
            if isinstance(user_topics, str):
                user_topics = [t.strip() for t in user_topics.replace("[", "").replace("]", "").replace("'", "").replace('"', "").split(",")]
            elif isinstance(user_topics, dict): 
                # Should have been caught above, but if it came from explicit 'Topics' dict
                pass

            if isinstance(user_topics, list):
                for t in user_topics:
                    if isinstance(t, str) and t.strip():
                        aliases_tuples.add((t.strip(), ""))
        
        return list(aliases_tuples)
    
    async def _match_alias_to_topic(self, alias: str, existing_topics: dict) -> str:
        """
        LLM decide si el alias es sin√≥nimo de alg√∫n topic existente.
        Retorna el nombre del topic si es sin√≥nimo, o None si es nuevo.
        """
        if not existing_topics:
            return None
        
        # Primero check exacto (case-insensitive)
        alias_lower = alias.lower().strip()
        for topic_id, topic_data in existing_topics.items():
            # Check nombre del topic
            if topic_data.get("name", "").lower().strip() == alias_lower:
                return topic_data.get("name")
            # Check aliases existentes
            for existing_alias in topic_data.get("aliases", []):
                if existing_alias.lower().strip() == alias_lower:
                    return topic_data.get("name")
        
        # Si no hay match exacto, usar LLM para matching sem√°ntico
        topics_list = []
        for tid, tdata in existing_topics.items():
            topics_list.append({
                "name": tdata.get("name", tid),
                "aliases": tdata.get("aliases", [])
            })
        
        prompt = f"""
        Alias del usuario: "{alias}"

        Topics existentes:
        {json.dumps(topics_list, ensure_ascii=False, indent=2)}

        REGLAS:
        - IA = Inteligencia Artificial = AI = Artificial Intelligence ‚Üí SIN√ìNIMOS
        - genAI ‚â† IA (relacionados pero NO sin√≥nimos, crear topic nuevo)
        - Geopol√≠tica = geopolitica = Geopolitics ‚Üí SIN√ìNIMOS
        - Macroeconom√≠a = macroeconomia = Macro = Econom√≠a Global ‚Üí SIN√ìNIMOS

        ¬øEste alias es SIN√ìNIMO de alg√∫n topic existente?
        
        Responde JSON:
        {{"match": "nombre_del_topic" o null}}
        """
        
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}]
            )
            result = _extract_json(response.choices[0].message.content)
            return result.get("match")
        except Exception as e:
            logger.warning(f"Error en matching LLM para '{alias}': {e}")
            return None
    
    async def _sync_aliases_with_topics(self, user_aliases_tuples: list, topics_data: dict) -> dict:
        """
        Sincroniza aliases de usuarios (y sus contextos) con topics existentes.
        - Agrega descripciones de usuarios al campo 'user_contexts' del topic.
        - Crea topics nuevos si no existen.
        """
        # RESET user_contexts for all topics (to ensure freshness from active users)
        for t_info in topics_data.values():
            t_info["user_contexts"] = []

        for alias, desc in user_aliases_tuples:
            found = False
            
            # 1. Buscar si el alias ya existe en alg√∫n topic
            matched_topic = None
            for topic_id, topic_info in topics_data.items():
                # Check exact alias match
                if alias in topic_info.get("aliases", []):
                    matched_topic = topic_info
                    found = True
                    break
                # Check topic name match
                if alias.lower() == topic_info.get("name", "").lower():
                    matched_topic = topic_info
                    found = True
                    if alias not in topic_info.get("aliases", []):
                        topic_info.setdefault("aliases", []).append(alias)
                    break
            
            # 2. Si no encontrado, intentar MATCH SEM√ÅNTICO (LLM)
            if not found:
                matched_topic_name = await self._match_alias_to_topic(alias, topics_data)
                
                if matched_topic_name:
                    for topic_id, topic_info in topics_data.items():
                        if topic_info.get("name") == matched_topic_name:
                            topic_info.setdefault("aliases", []).append(alias)
                            matched_topic = topic_info
                            logger.info(f"üîó Alias '{alias}' a√±adido a topic '{matched_topic_name}'")
                            found = True
                            break
            
            # 3. Si sigue sin encontrar, CREAR NUEVO
            if not found:
                topic_id = alias.lower().replace(" ", "_")
                topics_data[topic_id] = {
                    "name": alias,
                    "aliases": [alias],
                    "categories": [],
                    "noticias": [],
                    "user_contexts": []
                }
                matched_topic = topics_data[topic_id]
                logger.info(f"‚ûï Nuevo topic creado para alias '{alias}'")
            
            # 4. AGREGAR CONTEXTO DE USUARIO (Si existe descripci√≥n)
            if matched_topic and desc:
                # Evitar duplicados exactos de contexto
                if desc not in matched_topic.get("user_contexts", []):
                    matched_topic.setdefault("user_contexts", []).append(desc)

        return topics_data
    
    def _normalize_id(self, name: str) -> str:
        """Convierte nombre a ID normalizado"""
        id_str = name.lower().strip()
        id_str = re.sub(r'[^a-z√°√©√≠√≥√∫√º√±0-9\s]', '', id_str)
        id_str = re.sub(r'\s+', '_', id_str)
        return id_str
    
    def _load_topics_json(self) -> dict:
        """Carga topics.json de GCS o local"""
        try:
            # Fix: Use get_topics() which returns parsed JSON (list or dict)
            data = self.gcs.get_topics()
            if data:
                if isinstance(data, list):
                    return {self._normalize_id(t.get("name", t.get("id", ""))): t for t in data}
                return data
        except Exception as e:
            logger.error(f"‚ùå Error cargando topics desde GCS: {e}")
        
        # Fallback local
        local_path = os.path.join(os.path.dirname(__file__), "..", "data", "topics.json")
        if os.path.exists(local_path):
            with open(local_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, list):
                    return {self._normalize_id(t.get("name", t.get("id", ""))): t for t in data}
                return data
        return {}
    
    def _save_topics_json(self, data: dict):
        """Guarda topics.json en GCS y local"""
        topics_list = list(data.values())
        json_str = json.dumps(topics_list, ensure_ascii=False, indent=2)
        local_path = os.path.join(os.path.dirname(__file__), "..", "data", "topics.json")
        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        with open(local_path, "w", encoding="utf-8") as f:
            f.write(json_str)
        self.gcs.save_topics(topics_list)
    
    async def _assign_categories(self, topic_name: str) -> list:
        """Usa gpt-5-nano para asignar 2 categor√≠as"""
        categories_str = ", ".join(VALID_CATEGORIES)
        prompt = f"""
        Eres un clasificador. Dado el topic "{topic_name}", elige exactamente 2 categor√≠as de esta lista:
        {categories_str}
        
        Responde SOLO con un JSON: {{"categories": ["Cat1", "Cat2"]}}
        """
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
            )
            result = json.loads(response.choices[0].message.content)
            cats = result.get("categories", [])[:2]
            return [c for c in cats if c in VALID_CATEGORIES][:2] or ["General", "Sociedad"]
        except Exception as e:
            logger.error(f"Error asignando categor√≠as: {e}")
            return ["General", "Sociedad"]
    
    def _get_articles_for_categories(self, categories: list) -> list:
        """Busca art√≠culos en GCS din√°micamente seg√∫n la √∫ltima ejecuci√≥n"""
        
        # Calcular ventana din√°mica
        hours_limit = 24.0 # Default si no hay estado previo
        if hasattr(self, 'last_run_time') and self.last_run_time:
            delta = datetime.now() - self.last_run_time
            hours_limit = delta.total_seconds() / 3600
            hours_limit += 0.5 # Buffer de 30 mins
            
            # Cap limits
            hours_limit = max(0.1, min(hours_limit, 48.0)) # Min 6 min, Max 48h
            
        logger.info(f"üì° Buscando art√≠culos (Ventana din√°mica: {hours_limit:.2f}h)...")

        all_articles = []
        for cat in categories:
            articles = self.gcs.get_articles_by_category(cat, hours_limit=hours_limit)
            all_articles.extend(articles)
        # Deduplicar por URL
        seen = set()
        unique = []
        for a in all_articles:
            url = a.get("url", a.get("link", ""))
            if url and url not in seen:
                seen.add(url)
                unique.append(a)
        return unique
    
    async def _filter_relevant(self, topic: str, articles: list, user_contexts: list = None) -> list:
        """Filtra art√≠culos relevantes con gpt-5-nano"""
        if not articles:
            return []
        
        # Filtrar art√≠culos muy antiguos (m√°s de 24h)
        from datetime import datetime, timedelta
        cutoff = datetime.now() - timedelta(hours=24)
        fresh_articles = []
        for a in articles:
            pub_date = a.get("published_at")
            if isinstance(pub_date, str):
                try:
                    pub_date = datetime.fromisoformat(pub_date.replace("Z", "+00:00"))
                    if pub_date.replace(tzinfo=None) >= cutoff:
                        fresh_articles.append(a)
                except:
                    fresh_articles.append(a)  # Si no puede parsear, incluir
            else:
                fresh_articles.append(a)
        
        if not fresh_articles:
            logger.info(f"‚è∞ {topic}: 0 art√≠culos frescos (todos > 48h)")
            return []
        
        # Increase limit to allow more candidates for merging
        articles = fresh_articles[:60]
        
        articles_text = ""
        for i, a in enumerate(articles):
            snippet = (a.get("content") or a.get("description") or "")[:200]
            # INCLUIR FUENTE PARA QUE EL LLM PUEDA FILTRAR POR MEDIO SI EL USUARIO LO PIDE
            source = a.get("source_name", "Desconocido")
            articles_text += f"ID {i}: [FUENTE: {source}] {a.get('title')} | {snippet}\n"
        
        # Build User Context String for Optimized Filtering
        context_str = ""
        if user_contexts:
             # Clean and deduplicate contexts
             cleaned_contexts = [str(c).strip() for c in user_contexts if c and len(str(c).strip()) > 3]
             # Get unique contexts
             cleaned_contexts = list(set(cleaned_contexts))
             
             if cleaned_contexts:
                 # Limit to avoid huge prompts
                 list_str = "\n".join([f"- {c}" for c in cleaned_contexts[:20]]) 
                 context_str = (
                     f"\n‚ö†Ô∏è INTERESES ESPEC√çFICOS DE LOS USUARIOS (CR√çTICO PARA AHORRO DE COSTES):\n"
                     f"{list_str}\n"
                     f"INSTRUCCI√ìN DE FILTRADO: Solo selecciona noticias que RELEVANTES para estos intereses espec√≠ficos.\n"
                     f"SI UNA NOTICIA ES DEL TEMA '{topic}' PERO NO ENCAJA CON NING√öN INTER√âS ESPEC√çFICO, DESC√ÅRTALA (excepto Breaking News masivo).\n"
                 )

        prompt = f"""
        Eres un FILTRO DE RELEVANCIA inteligente para el topic: "{topic}".
        {context_str}
        Tu trabajo: Identificar noticias RELACIONADAS con "{topic}".
        
        INSTRUCCI√ìN SOBRE FUENTES Y MEDIOS:
        - Si el topic menciona expl√≠citamente una fuente (ej: "Pol√≠tica de El Confidencial"), SELECCIONA SOLO noticias de esa fuente.
        - Si el topic es gen√©rico, ignora la fuente y c√©ntrate en el contenido.
        
        ENFOQUE PARA TOPICS CIENT√çFICOS/T√âCNICOS (ej: f√≠sica cu√°ntica, IA, blockchain):
        - S√â INCLUSIVO: acepta investigaciones, papers, descubrimientos, avances
        - Acepta temas relacionados (f√≠sica cu√°ntica ‚Üí mec√°nica cu√°ntica, computaci√≥n cu√°ntica, entrelazamiento)
        - Acepta noticias de universidades, laboratorios, centros de investigaci√≥n
        - La palabra clave o tema debe aparecer o ser claramente impl√≠cito
        
        ENFOQUE PARA TOPICS DE ENTRETENIMIENTO/DEPORTE (ej: F1, Real Madrid):
        - Acepta noticias del equipo, competici√≥n, fichajes, partidos, declaraciones
        - Acepta contenido relacionado (ej: F1 ‚Üí carreras, pilotos, equipos, FIA, circuitos)
        
        RECHAZAR SIEMPRE:
        - Contenido publicitario/promocional/patrocinado
        - Reviews de productos de consumo (m√≥viles, gadgets, electrodom√©sticos)
        - Ofertas, descuentos, rebajas de tiendas
        - "Mejores productos", "gu√≠as de compra"
        - Cobertura en directo sin sustancia
        
        ACEPTAR:
        - Noticias informativas serias
        - Investigaci√≥n cient√≠fica o t√©cnica
        - An√°lisis de profundidad
        - Eventos relevantes del sector
        - Declaraciones de expertos, empresas o instituciones
        
        NOTICIAS A EVALUAR:
        {articles_text}
        
        Responde JSON con los IDs relevantes para "{topic}":
        {{"relevant_ids": [0, 2, 5]}}
        
        Si NINGUNA es relevante: {{"relevant_ids": []}}
        """
        
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
            )
            result = _extract_json(response.choices[0].message.content)
            ids = result.get("relevant_ids", [])
            # Devolver todas las relevantes - la deduplicaci√≥n fusionar√° fuentes
            return [articles[i] for i in ids if i < len(articles)]
        except Exception as e:
            logger.error(f"Error filtrando: {e}")
            return []
    
    async def _fetch_og_image(self, url: str) -> str:
        """Extrae og:image de una URL usando Open Graph, filtrando logos/iconos"""
        if not url:
            return ""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=8)) as response:
                    if response.status != 200:
                        return ""
                    html = await response.text()
                    # Buscar og:image con regex simple
                    match = re.search(r'<meta[^>]*property=["\']og:image["\'][^>]*content=["\']([^"\']+)["\']', html, re.IGNORECASE)
                    if not match:
                        match = re.search(r'<meta[^>]*content=["\']([^"\']+)["\'][^>]*property=["\']og:image["\']', html, re.IGNORECASE)
                    if match:
                        img_url = match.group(1)
                        
                        # Filtrar im√°genes que son logos, iconos o assets de redes sociales
                        skip_patterns = [
                            'logo', 'icon', 'favicon', 'avatar', 'brand',
                            'twitter.com', 'x.com', 'facebook.com', 'linkedin.com',
                            '/icons/', '/logos/', '/emoji/', '/emojis/',
                            'static.xx.', 'abs.twimg.com', 'pbs.twimg.com',
                            '.svg', '.ico', 'sprite', 'placeholder',
                            'default-', 'og-default', 'share-image', 'social-',
                            'msn.com/static', 'slashdot.org/~',  # Specific problematic sources
                        ]
                        
                        img_lower = img_url.lower()
                        for pattern in skip_patterns:
                            if pattern in img_lower:
                                return ""  # No usar esta imagen
                        
                        # Verificar que no sea una imagen muy peque√±a (par√°metros de URL)
                        if 'w=64' in img_lower or 'h=64' in img_lower or 'size=small' in img_lower:
                            return ""
                        
                        return img_url
        except Exception as e:
            pass  # Silently fail - image is optional
        return ""
    
    async def _fetch_article_content(self, url: str) -> str:
        """Extrae el texto principal de un art√≠culo web"""
        if not url:
            return ""
        try:
            async with aiohttp.ClientSession() as session:
                headers = {"User-Agent": "Mozilla/5.0 (compatible; NewsBot/1.0)"}
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=10), headers=headers) as response:
                    if response.status != 200:
                        return ""
                    html = await response.text()
                    
                    # Extraer texto de p√°rrafos <p>
                    paragraphs = re.findall(r'<p[^>]*>([^<]+(?:<[^/p][^>]*>[^<]*</[^p][^>]*>)*[^<]*)</p>', html, re.IGNORECASE | re.DOTALL)
                    text_content = " ".join(p.strip() for p in paragraphs if len(p.strip()) > 50)
                    
                    # Limpiar tags HTML residuales
                    text_content = re.sub(r'<[^>]+>', ' ', text_content)
                    text_content = re.sub(r'\s+', ' ', text_content).strip()
                    
                    # Limpiar basura HTML com√∫n de medios espa√±oles
                    garbage_patterns = [
                        r'Noticia\s+Relacionada[^.]*\.?',
                        r'Leer\s+(m[a√°]s|art[i√≠]culo\s+completo)[^.]*\.?',
                        r'Ver\s+(m[a√°]s|galer[i√≠]a|v[i√≠]deo)[^.]*\.?',
                        r'Suscr[i√≠]bete[^.]*\.?',
                        r'Newsletter[^.]*suscr[^.]*\.?',
                        r'estandar\s+No',
                        r'Premium\s+No',
                        r'Publicidad[^.]*\.?',
                    ]
                    for pattern in garbage_patterns:
                        text_content = re.sub(pattern, '', text_content, flags=re.IGNORECASE)
                    
                    text_content = re.sub(r'\s+', ' ', text_content).strip()
                    
                    return text_content[:3000]  # Limitar longitud
        except Exception as e:
            logger.debug(f"Error extrayendo contenido de {url}: {e}")
            return ""

    async def _redact_article(self, article: dict, topic: str) -> dict:
        """Redacta un articulo con gpt-5-nano"""
        title = article.get("title", "")
        content = article.get("content") or article.get("description") or ""
        url = article.get("url", article.get("link", ""))
        
        # FILTRO: Descartar noticias con contenido muy corto (solo titulares)
        # Si el RSS no tiene suficiente contenido, intentar extraerlo de la fuente
        MIN_CONTENT_LENGTH = 400  # caracteres minimos
        if len(content) < MIN_CONTENT_LENGTH and url:
            logger.info(f"üì• Contenido corto ({len(content)} chars), intentando extraer de la fuente...")
            fetched_content = await self._fetch_article_content(url)
            if fetched_content and len(fetched_content) >= MIN_CONTENT_LENGTH:
                content = fetched_content
                logger.info(f"   ‚úÖ Extra√≠do: {len(content)} chars")
            else:
                logger.info(f"‚è≠Ô∏è Descartando '{title[:40]}...' - contenido insuficiente")
                return None
        elif len(content) < MIN_CONTENT_LENGTH:
            logger.info(f"‚è≠Ô∏è Descartando '{title[:40]}...' - contenido muy corto ({len(content)} chars)")
            return None
        
        # Intentar obtener imagen del articulo o via Open Graph
        image = article.get("image_url", article.get("urlToImage", ""))
        if not image and url:
            image = await self._fetch_og_image(url)
        
        # Recopilar todas las fuentes (URLs) disponibles
        all_sources = [url] if url else []
        if article.get("extra_urls"):
            all_sources.extend(article.get("extra_urls", []))
        # Dedup
        all_sources = list(dict.fromkeys(all_sources))[:5]  # Max 5 fuentes
        
        prompt = f"""
        Eres un periodista experto. Redacta esta noticia DE CERO con tus propias palabras.
        
        Titulo original: {title}
        Contenido original: {content[:2000]}
        
        REGLAS CRITICAS DE REDACCI√ìN (OBLIGATORIAS):
        
        1. üö´ PROHIBIDO COPIAR Y PEGAR:
           - NO copies frases literales de la fuente.
           - Reescribe TODO el contenido con tu propio estilo.
        
        2. üö´ LIMPIEZA DE "BASURA" PERIOD√çSTICA:
           - ELIMINA prefijos del t√≠tulo como "EN DIRECTO", "√öltima Hora", "Noticia", etc. Crea un t√≠tulo NUEVO y atractivo.
           - ELIMINA del cuerpo frases como "En una videoconferencia desde...", "Seg√∫n informa el diario...", "Desde la redacci√≥n de...".
           - Redacta solo los HECHOS, sin meta-referencias a c√≥mo se obtuvo la noticia.
        
        3. FORMATO E IDIOMA:
           - IDIOMA: Espa√±ol peninsular. Traduce todo.
           - T√çTULO: Aterrizado, descriptivo y claro. Que se sepa EL SUJETO y LA ACCI√ìN. (Ej: "Apple lanza el nuevo iPhone" en vez de "El nuevo dispositivo ya est√° aqu√≠"). Emoji al principio.
           - RESUMEN: Exactamente entre 10 y 25 palabras.
           - NOTICIA: 150-250 palabras. Divide en parrafos con etiquetas <p>.
           - ESTILO: Informativo, directo y profesional.
           - IMPORTANTE: Usa <b>negrita</b> para 3 frases clave.

        4. üõ°Ô∏è FILTRO ANTI-AMBIG√úEDAD (CR√çTICO):
           - Si la noticia habla de "la compa√±√≠a", "el servicio", "la actualizaci√≥n" PERO NO NOMBRA expl√≠citamente a qui√©n se refiere (ej: falta el nombre de la empresa o producto), ¬°DESC√ÅRTALA!
           - Si la noticia depende de un contexto externo que no est√° en el texto ("Como dijimos ayer..."), ¬°DESC√ÅRTALA!
           - PARA DESCARTAR: Responde con un JSON vac√≠o: {{}}

        Responde JSON:
        {{
          "titulo": "Emoji T√≠tulo Descriptivo",
          "resumen": "Resumen breve...",
          "noticia": "<p>Contenido...</p>"
        }}
        o {{}} si es inv√°lida.
        """
        
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
            )
            result = _extract_json(response.choices[0].message.content)

            # VALIDACI√ìN: Si devolvi√≥ JSON vac√≠o o sin t√≠tulo, descartar.
            if not result or not result.get("titulo"):
                logger.info(f"‚è≠Ô∏è Descartando '{title[:40]}...' - AMBIGUA o INV√ÅLIDA (Filtro LLM)")
                return None
            
            return {
                "fecha_inventariado": datetime.now().isoformat(),
                "titulo": result.get("titulo"),
                "resumen": result.get("resumen", ""),
                "noticia": result.get("noticia", ""),
                "imagen_url": image,
                "fuentes": all_sources
            }
        except Exception as e:
            logger.error(f"Error redactando: {e}")
            return None

    # =========================================================================
    # RSS INGESTION HELPERS
    # =========================================================================
    async def _fetch_feed(self, session, url, timeout=20, retries=2):
        """Fetch a single feed with timeout and retries."""
        for attempt in range(retries + 1):
            try:
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=timeout)) as response:
                    if response.status != 200:
                        return None
                    return await response.text()
            except asyncio.TimeoutError:
                if attempt < retries:
                    await asyncio.sleep(1 * (attempt + 1))
                    continue
                return None
            except Exception:
                return None
        return None

    async def _fetch_and_parse_source(self, session, source):
        """Fetch and parse a single source."""
        url = source.get('url') or source.get('rss_url')
        category = source.get('category', 'General')
        name = source.get('name', url[:30] if url else 'Unknown')
        
        if not url: return [], "no_url"
        
        feed_content = await self._fetch_feed(session, url)
        if not feed_content: return [], "fetch_failed"
        
        try:
            feed = feedparser.parse(feed_content)
            entries = feed.entries[:15]
            
            if not entries: return [], "no_entries"
            
            articles = []
            for entry in entries:
                try:
                    link = entry.get('link')
                    if not link: continue
                    
                    title = entry.get('title', '')
                    summary = entry.get('summary', '') or entry.get('description', '')
                    
                    published_struct = entry.get('published_parsed')
                    if published_struct:
                        published_at = datetime(*published_struct[:6]).isoformat()
                    else:
                        published_at = datetime.now().isoformat()
                    
                    articles.append({
                        "url": link,
                        "title": title,
                        "content": summary[:1500],
                        "category": category,
                        "published_at": published_at,
                        "source_name": name
                    })
                except:
                    pass
            
            return articles, "ok" if articles else "parse_failed"
        except Exception as e:
            return [], f"error:{str(e)[:50]}"

    async def _ingest_all_rss(self):
        """Phase 0: Fetch from all RSS sources and save to GCS."""
        logger.info("üì• FASE 0: INGESTA RSS (Actualizando GCS...)")
        
        if not self.gcs.is_connected():
            logger.warning("‚ö†Ô∏è Sin conexi√≥n a GCS, saltando ingesta RSS.")
            return
        
        sources = self.gcs.get_sources()
        if not sources:
            logger.warning("‚ö†Ô∏è No hay sources.json en el bucket.")
            return

        logger.info(f"üì° Procesando {len(sources)} fuentes RSS...")
        all_new_articles = []
        BATCH_SIZE = 50
        
        async with aiohttp.ClientSession() as http_session:
            for batch_start in range(0, len(sources), BATCH_SIZE):
                batch_end = min(batch_start + BATCH_SIZE, len(sources))
                batch = sources[batch_start:batch_end]
                
                tasks = [self._fetch_and_parse_source(http_session, src) for src in batch]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result in results:
                    if isinstance(result, tuple):
                        articles, status = result
                        if articles:
                            all_new_articles.extend(articles)
        
        logger.info(f"üì∞ Total art√≠culos recolectados: {len(all_new_articles)}")
        
        if all_new_articles:
            added = self.gcs.merge_new_articles(all_new_articles)
            logger.info(f"‚úÖ Nuevos en GCS: {added}")
            self.gcs.cleanup_old_articles(hours=72)
        else:
            logger.info("üì≠ Sin nuevos art√≠culos en RSS.")


# Export function for main.py import
async def ingest_news():
    """Funci√≥n exportada para ser llamada desde main.py"""
    processor = HourlyProcessor()
    await processor.run()


async def main():
    await ingest_news()

if __name__ == "__main__":
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())
