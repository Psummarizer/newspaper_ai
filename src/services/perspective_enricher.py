"""
Perspective Enricher
=====================
Post-processing step for topics.json.

For each news item, finds other articles that cover the SAME story from
different sources (using Gemini embedding-based cosine-similarity clustering).

Adds to each article:
  - `perspectivas`: list of { medio, pais, idioma, resumen, url }
  - `community_note` (optional): short balanced note generated by Gemini Flash
    when 3+ sources with different geographic/editorial diversity cover the same story.

Design decisions vs. the JS original:
  - Uses `requests` (sync, simple) instead of fetch â€” called once in the async
    pipeline via asyncio.to_thread so it doesn't block the event loop.
  - Sources are matched from sources.json by domain; bias fields are NOT required
    (the existing sources.json lacks them). We expose country + language instead.
  - Union-Find clustering is kept identical to the JS implementation.
  - Community notes use Gemini Flash (cheapest, fastest) only when >=3 diverse
    sources cover the same event.
  - The enricher is IDEMPOTENT: running it twice on already-enriched data is safe.
"""

from __future__ import annotations

import json
import logging
import math
import os
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse

import requests

logger = logging.getLogger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIGURATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SIMILARITY_THRESHOLD = 0.83       # cosine threshold â€” slightly lower than JS (0.85)
                                   # because Spanish multi-source articles cluster tighter
MIN_PERSPECTIVES = 2               # need at least 2 articles to form a cluster
MAX_PERSPECTIVES = 8               # cap per article
COMMUNITY_NOTE_MIN_SOURCES = 3    # cluster size required for a community note
COMMUNITY_NOTE_MIN_DIVERSITY = 2  # at least N different countries required

EMBEDDING_MODEL = "gemini-embedding-001"
FLASH_MODEL = "gemini-2.0-flash-lite"

BATCH_SIZE = 100                   # embeddings per API call (Gemini batch limit)
BATCH_DELAY_S = 2.5                # seconds between embedding batches (rate-limit safety)

SOURCES_JSON_PATH = Path(__file__).parent.parent.parent / "data" / "sources.json"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SOURCE LOOKUP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _load_sources(path: Path = SOURCES_JSON_PATH) -> Dict[str, dict]:
    """
    Build a domain -> source metadata lookup from sources.json.
    Returns {} if the file is not found (enrichment will still run,
    perspectives just won't have rich metadata).
    """
    if not path.exists():
        logger.warning(f"sources.json not found at {path}. Perspectives will have basic metadata only.")
        return {}
    with open(path, "r", encoding="utf-8") as f:
        sources = json.load(f)
    lookup: Dict[str, dict] = {}
    for s in sources:
        domain = (s.get("domain") or "").lower().strip()
        if domain:
            lookup[domain] = {
                "name": s.get("name", domain),
                "country": s.get("country", "INT"),
                "language": s.get("language", "es"),
                "category": s.get("category", ""),
            }
    logger.info(f"ğŸ“š Sources lookup built: {len(lookup)} domains")
    return lookup


def _extract_domain(url: str) -> str:
    try:
        hostname = urlparse(url).hostname or ""
        return hostname.lower().replace("www.", "")
    except Exception:
        return ""


def _find_source_meta(article: dict, source_lookup: Dict[str, dict]) -> dict:
    """Find source metadata for an article by checking its `fuentes` URLs."""
    fuentes = article.get("fuentes") or []
    for url in fuentes:
        domain = _extract_domain(url)
        if not domain:
            continue
        if domain in source_lookup:
            meta = dict(source_lookup[domain])
            meta["url"] = url
            return meta
        # Try parent domain: feeds.elpais.com â†’ elpais.com
        parts = domain.split(".")
        if len(parts) > 2:
            parent = ".".join(parts[-2:])
            if parent in source_lookup:
                meta = dict(source_lookup[parent])
                meta["url"] = url
                return meta
    # Fallback
    fallback_url = fuentes[0] if fuentes else ""
    return {
        "name": _extract_domain(fallback_url) or "Desconocido",
        "country": "INT",
        "language": "es",
        "category": "",
        "url": fallback_url,
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EMBEDDINGS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _get_embeddings_batch(texts: List[str], api_key: str, max_retries: int = 3) -> List[List[float]]:
    """
    Call Gemini batchEmbedContents API for a list of texts (up to BATCH_SIZE).
    Returns a list of embedding vectors (float lists).
    """
    url = (
        f"https://generativelanguage.googleapis.com/v1beta"
        f"/models/{EMBEDDING_MODEL}:batchEmbedContents?key={api_key}"
    )
    payload = {
        "requests": [
            {
                "model": f"models/{EMBEDDING_MODEL}",
                "content": {"parts": [{"text": t[:2048]}]},
                "taskType": "SEMANTIC_SIMILARITY",
            }
            for t in texts
        ]
    }

    for attempt in range(max_retries + 1):
        try:
            resp = requests.post(url, json=payload, timeout=60)
            if resp.status_code == 200:
                data = resp.json()
                return [e["values"] for e in data["embeddings"]]
            if resp.status_code == 429 and attempt < max_retries:
                wait = (2 ** (attempt + 1)) * 10
                logger.warning(f"    â³ Rate limited (429). Waiting {wait}s (retry {attempt+1}/{max_retries})...")
                time.sleep(wait)
                continue
            resp.raise_for_status()
        except requests.exceptions.Timeout:
            if attempt < max_retries:
                time.sleep(5)
                continue
            raise
    return []


def _get_all_embeddings(texts: List[str], api_key: str) -> List[List[float]]:
    """Generate embeddings for all texts in batches."""
    all_embeddings: List[List[float]] = []
    total_batches = math.ceil(len(texts) / BATCH_SIZE)

    for i in range(0, len(texts), BATCH_SIZE):
        batch = texts[i : i + BATCH_SIZE]
        batch_num = i // BATCH_SIZE + 1
        logger.info(f"  ğŸ”¢ Embedding batch {batch_num}/{total_batches} ({len(batch)} texts)")
        embeddings = _get_embeddings_batch(batch, api_key)
        all_embeddings.extend(embeddings)
        if i + BATCH_SIZE < len(texts):
            time.sleep(BATCH_DELAY_S)

    return all_embeddings


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# COSINE SIMILARITY + UNION-FIND CLUSTERING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _cosine_similarity(a: List[float], b: List[float]) -> float:
    dot = sum(x * y for x, y in zip(a, b))
    norm_a = math.sqrt(sum(x * x for x in a))
    norm_b = math.sqrt(sum(x * x for x in b))
    if norm_a == 0 or norm_b == 0:
        return 0.0
    return dot / (norm_a * norm_b)


def _cluster_articles(
    embeddings: List[List[float]],
    threshold: float = SIMILARITY_THRESHOLD,
) -> List[List[int]]:
    """
    Union-Find clustering: groups indices of articles whose embeddings
    are above `threshold` cosine similarity.
    Returns only groups of size >= MIN_PERSPECTIVES.
    """
    n = len(embeddings)
    parent = list(range(n))

    def find(x: int) -> int:
        while parent[x] != x:
            parent[x] = parent[parent[x]]  # path compression
            x = parent[x]
        return x

    def union(x: int, y: int) -> None:
        rx, ry = find(x), find(y)
        if rx != ry:
            parent[rx] = ry

    for i in range(n):
        for j in range(i + 1, n):
            if _cosine_similarity(embeddings[i], embeddings[j]) >= threshold:
                union(i, j)

    groups: Dict[int, List[int]] = {}
    for i in range(n):
        root = find(i)
        groups.setdefault(root, []).append(i)

    return [g for g in groups.values() if len(g) >= MIN_PERSPECTIVES]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# COMMUNITY NOTE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _generate_community_note(
    cluster_articles: List[dict],
    source_lookup: Dict[str, dict],
    api_key: str,
) -> Optional[str]:
    """
    Generate a short, balanced community note via Gemini Flash.
    Only called when the cluster has >= COMMUNITY_NOTE_MIN_SOURCES articles
    from >= COMMUNITY_NOTE_MIN_DIVERSITY different countries.
    """
    perspectives = []
    for art in cluster_articles:
        meta = _find_source_meta(art, source_lookup)
        perspectives.append({
            "medio": meta["name"],
            "pais": meta["country"],
            "titulo": art.get("titulo", ""),
            "resumen": (art.get("resumen") or "")[:300],
        })

    countries = {p["pais"] for p in perspectives}
    if len(countries) < COMMUNITY_NOTE_MIN_DIVERSITY:
        return None

    prompt_lines = "\n".join(
        f"- {p['medio']} ({p['pais']}): \"{p['titulo']}\" â€” {p['resumen']}"
        for p in perspectives
    )
    prompt = (
        f"You are an impartial fact-checker. Analyze these {len(perspectives)} "
        f"perspectives on the same story and write a brief note (2-3 sentences) "
        f"that helps readers understand the different angles covered. "
        f"Be neutral. Reply ONLY with the note, no preamble.\n\n{prompt_lines}"
    )

    url = (
        f"https://generativelanguage.googleapis.com/v1beta"
        f"/models/{FLASH_MODEL}:generateContent?key={api_key}"
    )
    try:
        resp = requests.post(
            url,
            json={
                "contents": [{"parts": [{"text": prompt}]}],
                "generationConfig": {"maxOutputTokens": 200, "temperature": 0.3},
            },
            timeout=30,
        )
        if resp.status_code != 200:
            return None
        data = resp.json()
        return data["candidates"][0]["content"]["parts"][0]["text"].strip()
    except Exception as e:
        logger.debug(f"Community note generation failed: {e}")
        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAIN ENRICHMENT FUNCTION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def enrich_topics_with_perspectives(
    topics_data: dict,
    api_key: Optional[str] = None,
    generate_community_notes: bool = True,
    sources_path: Path = SOURCES_JSON_PATH,
) -> dict:
    """
    Enrich topics_data (the in-memory dict used by ingest_news.py) with
    `perspectivas` and optional `community_note` fields on each news article.

    Args:
        topics_data: The topics dict from HourlyProcessor (keys = topic_ids).
        api_key: Gemini API key. Falls back to GEMINI_API_KEY env var.
        generate_community_notes: Whether to call LLM for community notes.
        sources_path: Path to sources.json for bias/country metadata.

    Returns:
        The same dict, mutated in place (also returned for convenience).
    """
    api_key = api_key or os.getenv("GEMINI_API_KEY")
    if not api_key:
        logger.warning("âš ï¸  GEMINI_API_KEY not set â€” skipping perspective enrichment.")
        return topics_data

    source_lookup = _load_sources(sources_path)

    # â”€â”€ 1. Flatten all articles â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    flat_articles: List[dict] = []        # references to actual article dicts
    flat_meta: List[Tuple[str, int]] = [] # (topic_id, article_index in topic noticias)

    for topic_id, topic_info in topics_data.items():
        noticias = topic_info.get("noticias", [])
        for art_idx, article in enumerate(noticias):
            flat_articles.append(article)
            flat_meta.append((topic_id, art_idx))

    total = len(flat_articles)
    if total == 0:
        logger.info("No articles to enrich.")
        return topics_data

    logger.info(f"ğŸ”­ Perspective enrichment starting â€” {total} articles across {len(topics_data)} topics")

    # â”€â”€ 2. Generate embeddings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    texts = [
        f"{art.get('titulo', '')} {art.get('resumen', '')}"
        for art in flat_articles
    ]
    try:
        all_embeddings = _get_all_embeddings(texts, api_key)
    except Exception as e:
        logger.error(f"âŒ Embedding generation failed: {e}. Skipping enrichment.")
        return topics_data

    if len(all_embeddings) != total:
        logger.error(f"âŒ Embedding count mismatch ({len(all_embeddings)} vs {total}). Skipping.")
        return topics_data

    logger.info(f"âœ… {len(all_embeddings)} embeddings generated")

    # â”€â”€ 3. Group by category for intra-category comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # We only cluster articles within the same primary category to avoid
    # false matches across very different topic areas (e.g., Tech vs. Sports).
    by_category: Dict[str, List[int]] = {}
    for idx, (topic_id, _) in enumerate(flat_meta):
        cats = topics_data[topic_id].get("categories", [])
        primary_cat = cats[0] if cats else topics_data[topic_id].get("name", "General")
        by_category.setdefault(primary_cat, []).append(idx)

    # â”€â”€ 4. Cluster within each category + assign perspectives â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    total_clusters = 0
    total_enriched = 0
    total_notes = 0

    for category, global_indices in by_category.items():
        if len(global_indices) < MIN_PERSPECTIVES:
            continue

        cat_embeddings = [all_embeddings[i] for i in global_indices]
        groups = _cluster_articles(cat_embeddings)

        if groups:
            sizes = [len(g) for g in groups]
            logger.info(f"  [{category}] {len(groups)} clusters (sizes: {sizes})")
            total_clusters += len(groups)

        for local_group in groups:
            # Map local indices â†’ global flat indices â†’ actual article dicts
            global_group_indices = [global_indices[li] for li in local_group]
            group_articles = [flat_articles[gi] for gi in global_group_indices]

            # Build perspective entries for each article in the cluster
            perspectives: List[dict] = []
            for gi in global_group_indices:
                art = flat_articles[gi]
                meta = _find_source_meta(art, source_lookup)
                perspectives.append({
                    "medio": meta["name"],
                    "pais": meta.get("country", "INT"),
                    "idioma": meta.get("language", "es"),
                    "resumen": (art.get("resumen") or art.get("titulo") or "")[:300],
                    "url": meta.get("url") or (art.get("fuentes") or [""])[0],
                })

            # Community note (optional, only for diverse multi-source clusters)
            community_note: Optional[str] = None
            if (
                generate_community_notes
                and len(global_group_indices) >= COMMUNITY_NOTE_MIN_SOURCES
            ):
                community_note = _generate_community_note(group_articles, source_lookup, api_key)
                if community_note:
                    total_notes += 1

            # Assign to each article â€” exclude self (by domain), deduplicate
            for gi in global_group_indices:
                art = flat_articles[gi]
                self_domain = _extract_domain((art.get("fuentes") or [""])[0])
                seen_names: set = set()
                art["perspectivas"] = []
                for p in perspectives:
                    p_domain = _extract_domain(p["url"])
                    if p_domain and p_domain == self_domain:
                        continue  # skip self
                    if p["medio"] in seen_names:
                        continue  # deduplicate by name
                    seen_names.add(p["medio"])
                    art["perspectivas"].append(p)
                    if len(art["perspectivas"]) >= MAX_PERSPECTIVES:
                        break

                if community_note:
                    art["community_note"] = community_note
                total_enriched += 1

    logger.info(
        f"âœ… Enrichment done â€” clusters: {total_clusters}, "
        f"articles with perspectives: {total_enriched}/{total}, "
        f"community notes: {total_notes}"
    )
    return topics_data
